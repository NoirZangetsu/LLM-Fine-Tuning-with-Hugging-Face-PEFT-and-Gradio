
# ğŸ”¬ LLM Fine-Tuning with Hugging Face, PEFT, and Gradio

Bu proje, bÃ¼yÃ¼k dil modellerinin (LLM) dÃ¼ÅŸÃ¼k maliyetli ve verimli bir ÅŸekilde fine-tune edilmesini amaÃ§lamaktadÄ±r. Google Colab Ã¼zerinde Gradio arayÃ¼zÃ¼ desteÄŸiyle model eÄŸitimi ve test aÅŸamalarÄ± kullanÄ±cÄ± dostu bir ÅŸekilde entegre edilmiÅŸtir.

---

## ğŸš€ Proje Ã–zeti

Notebook, Hugging Face ekosistemini kullanarak `transformers`, `datasets`, `peft`, ve `accelerate` kÃ¼tÃ¼phaneleriyle aÅŸaÄŸÄ±daki iÅŸlemleri kapsamaktadÄ±r:

- Veriseti hazÄ±rlÄ±ÄŸÄ± ve iÅŸlenmesi
- PEFT (Parameter-Efficient Fine-Tuning) kullanÄ±larak modelin yeniden eÄŸitimi
- Gradio ile demo oluÅŸturulmasÄ±
- Hugging Face Hub'a model yÃ¼kleme desteÄŸi

---

## ğŸ§° KullanÄ±lan Teknolojiler ve KÃ¼tÃ¼phaneler

| KÃ¼tÃ¼phane           | AÃ§Ä±klama                                      |
|--------------------|-----------------------------------------------|
| `transformers`     | Hugging Face modelleri ve tokenizer desteÄŸi   |
| `datasets`         | Veriseti yÃ¼kleme ve iÅŸleme                    |
| `peft`             | Verimli parameter tuning (LoRA, QLoRA)        |
| `accelerate`       | DonanÄ±m hÄ±zlandÄ±rmalÄ± eÄŸitim                  |
| `gradio`           | EtkileÅŸimli web arayÃ¼zÃ¼ oluÅŸturma             |
| `huggingface_hub`  | Model paylaÅŸÄ±mÄ± ve yÃ¼kleme                    |

---

## ğŸ“‚ Veriseti

> Not: Kendi verisetinizi kullanmak iÃ§in `datasets.load_dataset()` fonksiyonu Ã¼zerinden `.csv`, `.json` veya Hugging Face Dataset Hub kullanÄ±labilir.

Notebook ÅŸu anda [Ã¶rnek bir TÃ¼rkÃ§e verisetini] kullanmaktadÄ±r ve belirli bir konu Ã¼zerine (Ã¶rneÄŸin sÃ¼rdÃ¼rÃ¼lebilirlik, hukuk vb.) eÄŸitilmek Ã¼zere hazÄ±rlanmÄ±ÅŸtÄ±r.

---

## ğŸ§  EÄŸitim DetaylarÄ±

- **Model:** (Ã¶rneÄŸin) `deepseek-ai/deepseek-llm-7b`
- **Fine-Tuning YÃ¶ntemi:** LoRA (Low-Rank Adaptation)
- **EÄŸitim Platformu:** Google Colab (T4 / A100 GPU)
- **Batch Size:** Ayarlanabilir
- **Epochs:** 3 (ayarlanabilir)
- **Optimizer:** AdamW

---

## âš™ï¸ Ã‡alÄ±ÅŸtÄ±rma AdÄ±mlarÄ±

### 1. Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:
```bash
pip install transformers datasets accelerate peft gradio huggingface_hub
```

### 2. Notebook'u Ã§alÄ±ÅŸtÄ±rÄ±n:
Google Colab'da adÄ±m adÄ±m cell'leri takip ederek:

- Veriseti yÃ¼kleme
- Tokenizer ve model baÅŸlatma
- PEFT ile fine-tuning
- Gradio UI ile test etme
- Hugging Face'e model yÃ¼kleme

---

## ğŸŒ Gradio Demo

Notebook sonunda aÅŸaÄŸÄ±daki gibi bir demo arayÃ¼zÃ¼ oluÅŸturulmaktadÄ±r:

- Input: Serbest metin giriÅŸ alanÄ±
- Output: Model Ã§Ä±ktÄ±sÄ±
- KullanÄ±m: Web Ã¼zerinden kolayca test edilebilir

---

## ğŸ“ˆ SonuÃ§lar ve DeÄŸerlendirme

EÄŸitim sonrasÄ± model, Ã¶zellikle TÃ¼rkÃ§e dilinde belirli gÃ¶revlerde baÅŸarÄ±lÄ± sonuÃ§lar vermektedir. Ek olarak, LoRA tekniÄŸi sayesinde dÃ¼ÅŸÃ¼k VRAM kullanÄ±mÄ±yla yÃ¼ksek doÄŸruluk elde edilmiÅŸtir.

> TensorBoard veya Weights & Biases entegrasyonu ile daha detaylÄ± takip eklenebilir.

---

## ğŸ“¤ Modeli Hugging Face'e YÃ¼kleme

```python
from huggingface_hub import login
login(token="hf_...")  # Hugging Face Access Token

trainer.push_to_hub()
tokenizer.push_to_hub()
```

---

## ğŸ¤ KatkÄ± SaÄŸlama

Her tÃ¼rlÃ¼ katkÄ±ya aÃ§Ä±ÄŸÄ±z! LÃ¼tfen `pull request` gÃ¶nderin veya `issue` aÃ§Ä±n. Yeni veriseti entegrasyonu veya model denemeleri yapÄ±labilir.

---

## ğŸ“œ Lisans

Bu proje MIT LisansÄ± ile lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in `LICENSE` dosyasÄ±nÄ± inceleyin.

---

## ğŸ‘¤ Yazar

**BAU**  
AI | NLP | LLM Training | Hugging Face & Transformers  
GitHub: [github.com/NoirZangetsu](https://github.com/NoirZangetsu)
